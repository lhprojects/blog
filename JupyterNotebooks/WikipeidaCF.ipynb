{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#读取数据\" data-toc-modified-id=\"读取数据-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>读取数据</a></span></li><li><span><a href=\"#反查表\" data-toc-modified-id=\"反查表-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>反查表</a></span></li><li><span><a href=\"#链接到本页的页也链接了\" data-toc-modified-id=\"链接到本页的页也链接了-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>链接到本页的页也链接了</a></span></li><li><span><a href=\"#这些页也链接了本页的链接的页\" data-toc-modified-id=\"这些页也链接了本页的链接的页-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>这些页也链接了本页的链接的页</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据\n",
    "\n",
    "读取已经解析好的维基百科数据。参见PageRank笔记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(\"pass1.5.data\", \"rb\") as f:\n",
    "    page_titles, redirect_map = pickle.load(f)\n",
    "\n",
    "page_title_indices = {}\n",
    "for i, page_title in enumerate(page_titles):\n",
    "    page_title_indices[page_title] = i\n",
    "    \n",
    "    \n",
    "def get_index_from_title(title):\n",
    "    while True:\n",
    "        if title in page_title_indices:\n",
    "            return page_title_indices[title]\n",
    "        \n",
    "        # don't use capitalize, it we lower the first char of a name\n",
    "        ctitle = title[:1].upper() + title[1:]\n",
    "        if ctitle != title:\n",
    "            if ctitle in page_title_indices:\n",
    "                return page_title_indices[ctitle]\n",
    "            \n",
    "        if title in redirect_map:\n",
    "            title_ = redirect_map[title]\n",
    "            # this is not a full dection of loop, but it's work\n",
    "            if title_ == title or title_ == ctitle:\n",
    "                break\n",
    "            title = title_\n",
    "        elif ctitle in redirect_map:\n",
    "            title_ = redirect_map[ctitle]\n",
    "            if title_ == title or title_ == ctitle:\n",
    "                break\n",
    "            title = title_\n",
    "        else:\n",
    "            break\n",
    "                        \n",
    "def get_title_from_index(index):\n",
    "    return page_titles[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles: 6047510\n",
      "Links: 170622101\n"
     ]
    }
   ],
   "source": [
    "links = np.load(\"pass2.links.npy\")\n",
    "page_n_links = np.load(\"pass2.page_n_links.npy\")\n",
    "\n",
    "assert(len(page_n_links) == len(page_title_indices))\n",
    "assert(page_n_links.sum() == len(links))\n",
    "\n",
    "print(\"Articles:\", len(page_n_links))\n",
    "print(\"Links:\", len(links))\n",
    "\n",
    "page_start = np.zeros(len(page_n_links), dtype=np.int)\n",
    "page_start[1:] = np.cumsum(page_n_links)[:-1]\n",
    "\n",
    "\n",
    "def get_links_from_title(title):\n",
    "    index = get_index_from_title(title)\n",
    "    if index is not None:\n",
    "        s = page_start[index]\n",
    "        n = page_n_links[index]\n",
    "        return [get_title_from_index(i) for i in links[s:s+n] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Logarithmic scale', 'Algorithm', 'Google Search', 'Web page', 'Web search engine', 'Larry Page', 'Google Patents', 'Network theory', 'Weighting', 'Hyperlink', 'Set (abstract data type)', 'World Wide Web', 'Link building', 'Webgraph', 'CNN', 'USA.gov', 'Recursion', 'Backlink', 'HITS algorithm', 'Jon Kleinberg', 'Teoma', 'Ask.com', 'CLEVER project', 'TrustRank', 'Google Hummingbird', 'Eigenvalues and eigenvectors', 'Scientometrics', 'Thomas L. Saaty', 'Analytic hierarchy process', 'Cognitive model', 'Baidu', 'Robin Li', 'The New York Times', 'Forbes', 'Sergey Brin', 'Stanford University', 'Rajeev Motwani', 'Terry Winograd', 'Google', 'Software patent', 'Citation analysis', 'Eugene Garfield', 'Hyper Search', 'Massimo Marchiori', 'University of Padua', 'Probability distribution', 'Matt Cutts', 'Markov chain', 'URL', 'Adjacency matrix', 'Stochastic matrix', 'Eigenvector centrality', 'Eigengap', 'Expected value', 'Wikipedia', 'Link farm', 'Trade secret', 'Power iteration', 'Steady state', 'Identity matrix', 'Perron–Frobenius theorem', 'Apache Spark', 'MATLAB', 'GNU Octave', 'Python (programming language)', 'Graph (abstract data type)', 'Bipartite graph', 'Random walk', 'Distributed algorithm', 'Google Toolbar', 'Danny Sullivan (technologist)', 'Search engine results page', 'Moz (marketing software)', 'Search engine optimization', 'Google Maps', 'Google Directory', 'HTTP 302', 'Meta element', 'Website spoofing', 'Nofollow', 'HTML attribute', 'Gaming the system', 'Citation', 'Sociometry', 'Attention economy', 'Institute for Scientific Information', 'Impact factor', 'Eigenfactor', 'SCImago Journal Rank', 'Neuroscience', 'Neuron', 'Twitter', 'Swiftype', 'Web crawler', 'Blogosphere', 'Scale-free network', 'Lexical semantics', 'Word-sense disambiguation', 'Semantic similarity', 'WordNet', 'Synonym ring', 'Link relation', 'Blog', 'Spamdexing', 'Spam in blogs', 'Search engine optimization metrics', 'Google Chrome', 'Domain Authority', 'EigenTrust', 'Google bombing', 'Google matrix', 'Google Panda', 'Google Penguin', 'VisualRank', 'Hilltop algorithm', 'Katz centrality', 'SimRank', 'CheiRank', 'Attention inequality']\n"
     ]
    }
   ],
   "source": [
    "print(get_links_from_title(\"PageRank\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反查表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inverse_table():\n",
    "        \n",
    "    n_pages = len(page_n_links)\n",
    "    inverse_link = [[] for i in range(n_pages)]\n",
    "    \n",
    "    for lk in range(n_pages):        \n",
    "        n_links = page_n_links[lk]\n",
    "        start = page_start[lk]\n",
    "        \n",
    "        for j in range(start, start+n_links): \n",
    "            this = links[j]            \n",
    "            inverse_link[this].append(lk)\n",
    "    \n",
    "    global links_inverse\n",
    "    global page_n_links_inverse\n",
    "    global page_start_inverse\n",
    "    links_inverse = np.empty(shape=len(links), dtype=np.int)\n",
    "    page_n_links_inverse = np.empty(shape=len(page_n_links), dtype=np.int)\n",
    "    \n",
    "    l = 0\n",
    "    for i, lks in enumerate(inverse_link):\n",
    "        page_n_links_inverse[i] = len(lks)\n",
    "        if len(lks):\n",
    "            links_inverse[l:l+len(lks)] = lks\n",
    "            l += len(lks)\n",
    "    \n",
    "    page_start_inverse = np.zeros(len(page_n_links_inverse), dtype=np.int)\n",
    "    page_start_inverse[1:] = np.cumsum(page_n_links_inverse)[:-1]\n",
    "    \n",
    "    assert page_n_links_inverse.sum() == len(links_inverse)\n",
    "    \n",
    "make_inverse_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_links_from_title_inverse(title):\n",
    "    index = get_index_from_title(title)\n",
    "    if index is not None:\n",
    "        s = page_start_inverse[index]\n",
    "        n = page_n_links_inverse[index]\n",
    "        return [get_title_from_index(i) for i in links_inverse[s:s+n] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Carl Linnaeus', 'Google Search', 'History of the Internet', 'List of algorithms', 'Web crawler', 'Spamdexing', 'Hyperlink', 'South East England', 'Advogato', 'Markov chain', 'Larry Page', 'Sergey Brin', \"Bradford's law\", 'Search engine optimization', 'Google Answers', 'Spectrum of a matrix', 'Link farm', 'Google bombing', 'PR', 'MediaWiki', 'Google (verb)', '1996 in science', 'Newsgroup spam', 'Wassily Leontief', 'Timeline of algorithms', 'Relevance', 'Fixed point (mathematics)', 'Global Multimedia Protocols Group', 'Thomas L. Saaty', 'Spam in blogs', 'Backlink', 'MozDex', 'Google Toolbar', 'Automatic summarization', 'Webometrics', 'Affiliate marketing', 'Document retrieval', 'Network theory', 'Web mining', 'List of eponyms (L–Z)', 'Trust metric', 'List of University of California, Berkeley alumni', 'VoIP spam', 'Findability', 'Google', 'Bibliometrics', 'Citation analysis', 'Jon Kleinberg', 'Eugene Garfield', 'Full-text search', 'Referrer spam', 'Centrality', 'Ranking', 'List of Google products', 'Perron–Frobenius theorem', 'Web 2.0', 'Raph Levien', 'Greg Errico', 'Baidu', 'OkCupid', 'Copywriting', 'HITS algorithm', 'IEEE Transactions on Information Theory', 'Tag cloud', 'Tf–idf', 'Eigenvalues and eigenvectors', 'TrustRank', 'Andrew Clausen', 'Power-knowledge', 'Spam blog', 'Sports rating system', 'Lanczos algorithm', 'Sakura HyperMedia Desktop', 'Organic search', 'Greggs', 'Scraper site', 'Attention economy', 'Domains by Proxy', 'Matt Cutts', 'Eigenvector centrality', 'Web ranking', 'Nofollow', 'Jeremy Zawodny', 'Reputation system', 'Web search engine', 'Spam mass', \"Shepard's Citations\", 'History of the World Wide Web', 'History of Google', 'Journal of Biological Chemistry', 'Wikipedia', 'Robin Li', 'Google Personalized Search', 'Consensus (computer science)', 'Power iteration', 'Massimo Marchiori', 'Mark Burgess (computer scientist)', 'Search engine technology', 'Hilltop algorithm', 'Social search', 'Sepandar Kamvar', 'Linkback', 'Saipan Sucks', 'PR2', 'PR5', 'S-rank', 'Article marketing', 'Criticism of Google', 'Journal ranking', 'Domain name drop list', 'Focused crawler', 'Hyper Search', 'Noviforum', 'Enterprise search', 'Proxy voting', 'WikiPilipinas', 'SocialRank', 'Eigendecomposition of a matrix', 'Reputation capital', 'Link building', 'Kaltix', 'Network science', 'Rajeev Motwani', 'VisualRank', 'Swoogle', 'Rebelion.org', 'Teachstreet', 'Operation Clambake', 'DMOZ', 'SimRank', 'Ranking (information retrieval)', 'Google matrix', 'Matrix (mathematics)', 'Richard J. Sullivan', 'Seznam.cz', 'DeepPeep', 'Eigenfactor', 'Stanford Digital Library Project', 'NDepend', 'Googlization', 'SCImago Journal Rank', 'Legal information retrieval', 'Learning to rank', 'Outline of Google', 'Quora', 'MediaWiki extension', 'Convergent Functional Genomics', 'Learning analytics', 'Leo Katz (statistician)', 'Lee–Carter model', 'Vitaly Borker', 'CheiRank', \"Rexer's Annual Data Miner Survey\", 'Google juice', 'Katz centrality', 'Yandex Search', 'Webgraph', 'Academic visibility', 'GraphLab', 'Marketing and artificial intelligence', 'Google Penguin', 'Moz (marketing software)', 'SALSA algorithm', 'Google penalty', 'EdgeRank', 'Entity linking', 'PageRank algorithm in biochemistry', 'Google Hummingbird', \"Who's Bigger?\", 'Timeline of Google Search', 'Timeline of web search engines', 'Apache Spark', 'Kraliçe', 'Mükemmel', 'Rasim Alguliyev', 'Yooreeka', 'Google Pigeon', 'Soumen Chakrabarti', 'Multidimensional network', 'Technology transfer in computer science', 'Bianconi–Barabási model', '9 Algorithms That Changed the Future', \"List of Google April Fools' Day jokes\", 'Search engine optimization metrics', 'RankBrain', 'Domain Authority', 'European Union vs. Google', 'Internet manipulation', 'Local search engine optimisation', 'Outline of machine learning', 'Google Fred', 'WordLift', 'Alessandro Strumia', 'Search engine privacy', 'Truth discovery', 'Amy Langville', 'Random surfing model']\n"
     ]
    }
   ],
   "source": [
    "print(get_links_from_title_inverse(\"PageRank\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 链接到本页的页也链接了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def collect_links(i, links, page_n_links, page_start,\n",
    "                  links_inverse, page_n_links_inverse, page_start_inverse,\n",
    "                  links_):\n",
    "    \n",
    "    total = 0    \n",
    "    start1 = page_start_inverse[i]\n",
    "    n_pages1 = page_n_links_inverse[i]\n",
    "    \n",
    "    # all pages that links to this\n",
    "    for l1 in range(start1, start1 + n_pages1):\n",
    "        l1_ = links_inverse[l1]\n",
    "        \n",
    "        \n",
    "        start2 = page_start[l1_]\n",
    "        n_pages2 = page_n_links[l1_]\n",
    "        \n",
    "        # all pages that page(l1_) links to\n",
    "        for l2 in range(start2, start2+n_pages2):\n",
    "            l2_ = links[l2]\n",
    "            links_[total] = l2_\n",
    "            total += 1\n",
    "    return total\n",
    "    \n",
    "\n",
    "def itemitem(i, links, page_n_links, page_start,\n",
    "             links_inverse, page_n_links_inverse, page_start_inverse,\n",
    "             links_ = None):\n",
    "    \n",
    "    if links_ is None:\n",
    "        links_  = np.empty(len(links), np.int)\n",
    "    \n",
    "    total = collect_links(i, links, page_n_links, page_start,\n",
    "                  links_inverse, page_n_links_inverse, page_start_inverse, links_)\n",
    "    \n",
    "    uns, uns_count = np.unique(links_[:total], return_counts=True)\n",
    "    uns_w = uns_count/(np.sqrt(page_n_links_inverse[uns])*np.sqrt(page_n_links_inverse[i]))\n",
    "    \n",
    "    indices = np.argsort(uns_w)[::-1][:10]\n",
    "    uns = uns[indices]\n",
    "    return uns\n",
    "    \n",
    "def pages_link_to_this_page_also_link_to(this_page):\n",
    "    i = get_index_from_title(this_page)\n",
    "    uns = itemitem(i, links, page_n_links, page_start,\n",
    "                  links_inverse, page_n_links_inverse, page_start_inverse)\n",
    "    \n",
    "    for pid in uns:\n",
    "        page = get_title_from_index(pid)\n",
    "        print(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History of Beijing\n",
      "Beijing city fortifications\n",
      "Ji (state)\n",
      "Liu Bingzhong\n",
      "Taiye Lake\n",
      "Jingshan Park\n",
      "Khanbaliq\n",
      "Xicheng District\n",
      "Imperial City, Beijing\n",
      "Jicheng (Beijing)\n",
      "====\n",
      "PageRank\n",
      "HITS algorithm\n",
      "Spamdexing\n",
      "Backlink\n",
      "Google Panda\n",
      "Google Penguin\n",
      "TrustRank\n",
      "Katz centrality\n",
      "History of Google\n",
      "Search engine optimization\n",
      "====\n",
      "Liang (surname)\n",
      "Ge (surname)\n",
      "Mou (surname)\n",
      "Wat (surname)\n",
      "Fei (surname)\n",
      "Lian (surname)\n",
      "Ji (surname 嵇)\n",
      "Shuai\n",
      "Liang Siyong\n",
      "Di (surname)\n"
     ]
    }
   ],
   "source": [
    "pages_link_to_this_page_also_link_to(this_page=\"History of Beijing\")\n",
    "print(\"====\")\n",
    "pages_link_to_this_page_also_link_to(this_page=\"PageRank\")\n",
    "print(\"====\")\n",
    "pages_link_to_this_page_also_link_to(this_page=\"Liang (surname)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这些页也链接了本页的链接的页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def pages_this_page_link_to_also_linked_by(this_page):\n",
    "    i = get_index_from_title(this_page)\n",
    "    uns = itemitem(i, links_inverse, page_n_links_inverse, page_start_inverse,\n",
    "                  links, page_n_links, page_start)\n",
    "    \n",
    "    for pid in uns:\n",
    "        page = get_title_from_index(pid)\n",
    "        print(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History of Beijing\n",
      "Jicheng (Beijing)\n",
      "Beijing\n",
      "Nanjing (Liao dynasty)\n",
      "History of China\n",
      "Beijing city fortifications\n",
      "Timeline of Chinese history\n",
      "History of the Great Wall of China\n",
      "You Prefecture\n",
      "Zhongnanhai\n",
      "====\n",
      "PageRank\n",
      "TrustRank\n",
      "Search engine optimization\n",
      "Nofollow\n",
      "Backlink\n",
      "Hilltop algorithm\n",
      "Google matrix\n",
      "Spamdexing\n",
      "Anchor text\n",
      "HITS algorithm\n",
      "====\n",
      "Liang (surname)\n",
      "Mediacorp Subaru Impreza WRX Challenge\n",
      "Borders of Vietnam\n",
      "Macrotermes carbonarius\n",
      "Alstonia spatulata\n",
      "Box of Hope\n",
      "Gymnanthera oblonga\n",
      "Culex rubithoracis\n",
      "Miele Guide\n",
      "Asian-Oceanian Computing Industry Organization\n"
     ]
    }
   ],
   "source": [
    "pages_this_page_link_to_also_linked_by(this_page=\"History of Beijing\")\n",
    "print(\"====\")\n",
    "pages_this_page_link_to_also_linked_by(this_page=\"PageRank\")\n",
    "print(\"====\")\n",
    "pages_this_page_link_to_also_linked_by(this_page=\"Liang (surname)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
