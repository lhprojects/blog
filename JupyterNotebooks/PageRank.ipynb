{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#数据处理\" data-toc-modified-id=\"数据处理-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>数据处理</a></span><ul class=\"toc-item\"><li><span><a href=\"#数据概览\" data-toc-modified-id=\"数据概览-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>数据概览</a></span></li><li><span><a href=\"#维基百科数据xml-parser\" data-toc-modified-id=\"维基百科数据xml-parser-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>维基百科数据xml parser</a></span></li><li><span><a href=\"#Pass-1:-收集标题和重定向\" data-toc-modified-id=\"Pass-1:-收集标题和重定向-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Pass 1: 收集标题和重定向</a></span></li><li><span><a href=\"#预处理和统计\" data-toc-modified-id=\"预处理和统计-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>预处理和统计</a></span></li><li><span><a href=\"#编号\" data-toc-modified-id=\"编号-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>编号</a></span></li><li><span><a href=\"#Pass2:-链接\" data-toc-modified-id=\"Pass2:-链接-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Pass2: 链接</a></span><ul class=\"toc-item\"><li><span><a href=\"#从page正文中提取链接\" data-toc-modified-id=\"从page正文中提取链接-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>从page正文中提取链接</a></span></li></ul></li><li><span><a href=\"#title模式\" data-toc-modified-id=\"title模式-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>title模式</a></span></li></ul></li><li><span><a href=\"#PageRank\" data-toc-modified-id=\"PageRank-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>PageRank</a></span><ul class=\"toc-item\"><li><span><a href=\"#迭代算法\" data-toc-modified-id=\"迭代算法-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>迭代算法</a></span></li><li><span><a href=\"#应用\" data-toc-modified-id=\"应用-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>应用</a></span><ul class=\"toc-item\"><li><span><a href=\"#查询\" data-toc-modified-id=\"查询-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>查询</a></span></li><li><span><a href=\"#top-100\" data-toc-modified-id=\"top-100-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>top 100</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "debugging=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理\n",
    "## 数据概览\n",
    "我们下载了2020年4月1日英文维基百科的数据。数据由xml组成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia\n",
    "file_path=r\"..\\..\\..\\data\\enwiki-20200401-pages-articles-multistream.xml\\enwiki-20200401-pages-articles-multistream.xml\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    with open(\"./wiki_head.txt\", \"w\", encoding=\"utf-8\") as out:\n",
    "        out.write(f.read(50000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前50000字符见[此处](./wiki_head.txt)。\n",
    "\n",
    "整个数据集由页组成。有两种页，一种是重定向类型的，重定向的页的正文并不是真正的内容，另外一种是正文具有真正内容的页。\n",
    "每个都有一个名字空间，名字空间为0，表示这是一个通常的文章(Article)。名字空间除了文章，还有包括模板，讨论，分类等等。\n",
    "我们只读取表示通常页的文章。\n",
    "每个页都有一个标题(title)，一般来说每个页的标题并不相同，但是有极个别例外，我们在名字空间0中发现了两个重复标题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 维基百科数据xml parser\n",
    "因为数据巨大，我们采用流式解析器。\n",
    "对于极大的文件，采用流式解析器，这样可以保证速度和有限的内存，但是给编码带来一定复杂度。\n",
    "代码里包含一些小技巧来避坑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.sax\n",
    "\n",
    "\n",
    "class AbortException(Exception):\n",
    "    pass\n",
    "\n",
    "class StreamHandler(xml.sax.handler.ContentHandler):\n",
    "\n",
    "    def __init__(self, on_page=None, skip_n_pages=0):\n",
    "        self.cnt = 0\n",
    "        self.on_page = on_page\n",
    "        self.skip_n_pages = skip_n_pages\n",
    "        \n",
    "        # character may before any startElement        \n",
    "        self.name = None\n",
    "        self.title = None\n",
    "        self.redirect = None\n",
    "        self.ns = -1\n",
    "        self.text = None\n",
    "    \n",
    "    def startElement(self, name, attrs):\n",
    "        \n",
    "        if self.cnt < self.skip_n_pages:\n",
    "            return\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "            \n",
    "        if name == \"redirect\":\n",
    "            assert attrs.getLength() == 1\n",
    "            for n in attrs.getNames():\n",
    "                self.redirect = attrs.getValue(n)\n",
    "        elif name == \"title\":\n",
    "            self.title_ = []\n",
    "        elif name == \"ns\":\n",
    "            self.ns_ = []\n",
    "        elif name == \"text\":\n",
    "            self.text_ = []\n",
    "    \n",
    "\n",
    "\n",
    "    def endElement(self, name):\n",
    "        \n",
    "        if self.cnt < self.skip_n_pages:\n",
    "            if name == \"page\":\n",
    "                if self.cnt % 10000 == 0:\n",
    "                    print(\"\\rpages: \", self.cnt, end=\"\", flush=True)\n",
    "                self.cnt += 1\n",
    "            return\n",
    "            \n",
    "        \n",
    "        if name == \"title\":\n",
    "            self.title = \"\".join(self.title_)\n",
    "        elif name == \"ns\":\n",
    "            self.ns = int(\"\".join(self.ns_))\n",
    "        elif name == \"text\":\n",
    "            self.text  = \"\".join(self.text_)\n",
    "        elif name == \"page\":\n",
    "            \n",
    "            if self.on_page:\n",
    "                self.on_page(self)                \n",
    "                \n",
    "            self.title = None\n",
    "            self.redirect = None\n",
    "            self.ns = -1\n",
    "            self.text = None\n",
    "            \n",
    "            if self.cnt % 10000 == 0:\n",
    "                print(\"\\rpages: \", self.cnt, end=\"\", flush=True)\n",
    "                \n",
    "            self.cnt += 1\n",
    "            \n",
    "        self.name = None\n",
    "\n",
    "    def characters(self, content):\n",
    "        if self.cnt < self.skip_n_pages:\n",
    "            return\n",
    "        \n",
    "        # character may be invokde once for a line or for handured of characters\n",
    "        # we need to contact them in the `endElement` by ourselves        \n",
    "        if self.name == \"title\":                            \n",
    "            self.title_.append(content)\n",
    "        if self.name == \"ns\":                            \n",
    "            self.ns_.append(content)\n",
    "        elif self.name == \"text\":\n",
    "            self.text_.append(content)\n",
    "            \n",
    "            \n",
    "def parse_wiki(on_page, skip_n_pages = 0):\n",
    "    parser = xml.sax.make_parser()\n",
    "    handler = StreamHandler(on_page, skip_n_pages)\n",
    "    parser.setContentHandler(handler)\n",
    "        \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            parser.parse(f)\n",
    "        except AbortException:\n",
    "            pass\n",
    "    print(\"\\rpages: \", handler.cnt, end=\"\", flush=True)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass 1: 收集标题和重定向\n",
    "* 收集所有真正页的标题\n",
    "* 记录重定向页的映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pages:  19880000\n",
      "repeat title\n",
      "title:  Planned Parenthood v. Rounds\n",
      "len(page_titles) 5997620\n",
      "pages:  20100000\n",
      "repeat title\n",
      "title:  Caleb Baldwin\n",
      "len(page_titles) 6042047\n",
      "pages:  20128863\n",
      "1h 21min 17s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# exlude redirect page titles\n",
    "page_titles = []\n",
    "# include redirect page titles\n",
    "page_titles_set = set()\n",
    "# redirect page title -> real page title\n",
    "redirect_map = {}\n",
    "\n",
    "def get_page_titles():\n",
    "    \n",
    "    def on_page(self):\n",
    "        \n",
    "        # considering only articles\n",
    "        if self.ns != 0:\n",
    "            return\n",
    "         \n",
    "        # no title ?\n",
    "        if not self.title:\n",
    "            print(\"\\nno title\")\n",
    "            print(\"title: \", self.title)\n",
    "            print(\"len(page_titles)\", len(page_titles))\n",
    "            return\n",
    "            \n",
    "        if self.redirect is not None:\n",
    "            redirect_map[self.title] = self.redirect            \n",
    "            return\n",
    "                \n",
    "        # repeat title?        \n",
    "        if self.title in page_titles_set:\n",
    "            print(\"\\nrepeat title\")\n",
    "            print(\"title: \", self.title)                    \n",
    "            print(\"len(page_titles)\", len(page_titles))\n",
    "        else:\n",
    "            page_titles_set.add(self.title)\n",
    "            \n",
    "        page_titles.append(self.title)\n",
    "        \n",
    "        if debugging:\n",
    "            if self.cnt >10000:\n",
    "                raise AbortException()\n",
    "    \n",
    "    parse_wiki(on_page)\n",
    "    \n",
    "    with open(\"pass1  .data\", \"wb\") as f:\n",
    "        pickle.dump([page_titles, redirect_map], f)\n",
    "\n",
    "%timeit -r1 -n1 get_page_titles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理和统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real pages:  6047512\n",
      "repeat: Planned Parenthood v. Rounds\n",
      "repeat: Caleb Baldwin\n",
      "maxlen 250\n",
      "Cneoridium dumosum (Nuttall) Hooker F. Collected March 26, 1960, at an Elevation of about 1450 Meters on Cerro Quemazón, 15 Miles South of Bahía de Los Angeles, Baja California, México, Apparently for a Southeastward Range Extension of Some 140 Miles\n",
      "maxlen 251\n",
      "Protocol Amending the Agreements, Conventions and Protocols on Narcotic Drugs concluded at The Hague on 23 January 1912, at Geneva on 11 February 1925 and 19 February 1925, and 13 July 1931, at Bangkok on 27 November 1931 and at Geneva on 26 June 1936\n",
      "unique title real pages:  6047510\n"
     ]
    }
   ],
   "source": [
    "with open(\"pass1.data\", \"rb\") as f:\n",
    "    page_titles, redirect_map = pickle.load(f)\n",
    "\n",
    "def count():\n",
    "    global page_titles\n",
    "    \n",
    "    print(\"real pages: \", len(page_titles))    \n",
    "    max_len = 0\n",
    "    max_title = None\n",
    "    page_titles_ = []\n",
    "    page_titles_set = set()\n",
    "    for i in range(len(page_titles)):\n",
    "        title = page_titles[i]\n",
    "        \n",
    "        stitle = title.strip()\n",
    "        if stitle != title:\n",
    "            print(\"strip:\", stitle, title)\n",
    "        \n",
    "        if title in page_titles_set:\n",
    "            print(\"repeat:\", title)\n",
    "        else:\n",
    "            page_titles_set.add(title)\n",
    "            page_titles_.append(title)\n",
    "            \n",
    "        if len(title) > max_len:\n",
    "            max_len = len(title)\n",
    "            max_title = title\n",
    "        \n",
    "    page_titles = page_titles_\n",
    "    \n",
    "    print(\"maxlen\", max_len)\n",
    "    print(max_title)\n",
    "    \n",
    "    max_len = 0\n",
    "    for title in redirect_map:\n",
    "        if len(title) > max_len:\n",
    "            max_len = len(title)\n",
    "            max_title = title\n",
    "    print(\"maxlen\", max_len)\n",
    "    print(max_title)\n",
    "    print(\"unique title real pages: \", len(page_titles))        \n",
    "\n",
    "count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_title_indices = {}\n",
    "for i, page_title in enumerate(page_titles):\n",
    "    page_title_indices[page_title] = i\n",
    "    \n",
    "    \n",
    "def get_index_from_title(title):\n",
    "    while True:\n",
    "        if title in page_title_indices:\n",
    "            return page_title_indices[title]\n",
    "        \n",
    "        # don't use capitalize, it we lower the first char of a name\n",
    "        ctitle = title[:1].upper() + title[1:]\n",
    "        if ctitle != title:\n",
    "            if ctitle in page_title_indices:\n",
    "                return page_title_indices[ctitle]\n",
    "            \n",
    "        if title in redirect_map:\n",
    "            title_ = redirect_map[title]\n",
    "            # this is not a full dection of loop, but it's work\n",
    "            if title_ == title or title_ == ctitle:\n",
    "                break\n",
    "            title = title_\n",
    "        elif ctitle in redirect_map:\n",
    "            title_ = redirect_map[ctitle]\n",
    "            if title_ == title or title_ == ctitle:\n",
    "                break\n",
    "            title = title_\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "def print_index_from_title(title):\n",
    "    print(title)\n",
    "    while True:\n",
    "        if title in page_title_indices:\n",
    "            print(\"->\", page_title_indices[title])\n",
    "        \n",
    "        # don't use capitalize, it we lower the first char of a name\n",
    "        ctitle = title[:1].upper() + title[1:]\n",
    "        if ctitle != title:\n",
    "            if ctitle in page_title_indices:\n",
    "                print(\"->\", ctitle, \"->\", page_title_indices[ctitle])\n",
    "            \n",
    "        if title in redirect_map:\n",
    "            title_ = redirect_map[title]\n",
    "            # this is not a full dection of loop, but it's work\n",
    "            if title_ == title or title_ == ctitle:\n",
    "                break\n",
    "            title = title_\n",
    "            print(\"->\", title)\n",
    "        elif ctitle in redirect_map:\n",
    "            title_ = redirect_map[ctitle]\n",
    "            if title_ == title or title_ == ctitle:\n",
    "                break\n",
    "            title = title_\n",
    "            print(\"->\", title)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "def get_title_from_index(index):\n",
    "    return page_titles[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Anarchism\n"
     ]
    }
   ],
   "source": [
    "print(get_index_from_title(\"Anarchism\"))\n",
    "print(get_title_from_index(get_index_from_title(\"Anarchism\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass2: 链接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从page正文中提取链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t1']\n",
      "['t2']\n",
      "['t3']\n",
      "['t3']\n",
      "['t5', 't6']\n",
      "['t7']\n",
      "['t8']\n",
      "['t9']\n",
      "['t1']\n",
      "[]\n",
      "['t3 (a)']\n"
     ]
    }
   ],
   "source": [
    "#https://en.wikipedia.org/wiki/Wikipedia:Page_name\n",
    "# don't contiain #<>[\\]|{}_ in title\n",
    "# have at most 255 characters (indeed 255 bytes) in title\n",
    "# however we have to a larger value for this, to take into the namespace\n",
    "# we assume at most 100 character, for tag, and text of hyperlink\n",
    "\n",
    "import re\n",
    "title_pattern = re.compile(r'''\\[\\[:?([^#<>[\\]|{}_]{1,265})(?:#[^[\\]|]{0,100})?(?:\\|[^[\\]]{0,100})?\\]\\]''')\n",
    "#title_pattern = re.compile(r'''\\[\\[:?([^#<>[\\]|{}_]+)\\]\\]''')\n",
    "print(title_pattern.findall(\"[[t1]]\"))\n",
    "print(title_pattern.findall(\"[[t2#tag]]\"))\n",
    "print(title_pattern.findall(\"[[t3|text]]\"))\n",
    "print(title_pattern.findall(\"[[t3#tag|text]]\"))\n",
    "print(title_pattern.findall(\"[[t5#tag|text ]] [[t6|text]]\"))\n",
    "print(title_pattern.findall(\"[[:t7#tag|text]]\"))\n",
    "print(title_pattern.findall(\"[[[[t8]]\"))\n",
    "print(title_pattern.findall(\"[[t_ [[t9]]\"))\n",
    "print(title_pattern.findall(\"[[t1|aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa]]\"))\n",
    "print(title_pattern.findall(\"[[t2|aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa]]\"))\n",
    "print(title_pattern.findall(\"[[t3 (a)]]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pages:  19880000\n",
      "repeat title\n",
      "title:  Planned Parenthood v. Rounds\n",
      "len(page_titles) 6047510\n",
      "pages:  20100000\n",
      "repeat title\n",
      "title:  Caleb Baldwin\n",
      "len(page_titles) 6047510\n",
      "pages:  20128863\n",
      "1h 37min 25s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# links of all page, all links in one dimension\n",
    "links=[]\n",
    "# number of links of each page\n",
    "page_n_links = []\n",
    "\n",
    "# for debug\n",
    "paring_page_title = None\n",
    "paring_page_text = None\n",
    "page_titles_set = set()\n",
    "\n",
    "def get_page_links():\n",
    "    \n",
    "    global page_n_links\n",
    "    global links\n",
    "    def on_page(self):\n",
    "                        \n",
    "        global paring_page_title\n",
    "        global paring_page_text\n",
    "        \n",
    "        # considering only articles\n",
    "        if self.ns != 0:\n",
    "            return\n",
    "         \n",
    "        # no title ?\n",
    "        if not self.title:\n",
    "            print(\"\\nno title\")\n",
    "            print(\"title: \", self.title)\n",
    "            print(\"len(page_titles)\", len(page_titles))\n",
    "            return\n",
    "            \n",
    "        if self.redirect is not None:\n",
    "            return\n",
    "                \n",
    "        # repeat title?        \n",
    "        if self.title in page_titles_set:\n",
    "            print(\"\\nrepeat title\")\n",
    "            print(\"title: \", self.title)                    \n",
    "            print(\"len(page_titles)\", len(page_titles))\n",
    "            return\n",
    "        else:\n",
    "            page_titles_set.add(self.title)\n",
    "            \n",
    "        \n",
    "        paring_page_title = self.title\n",
    "        paring_page_text = self.text\n",
    "               \n",
    "        # if index of this title if correct\n",
    "        if self.title != page_titles[len(page_n_links)]:\n",
    "            print(self.title, \"!=\", page_titles[len(page_n_links)])\n",
    "            assert False                        \n",
    "            \n",
    "        page_n_links.append(0)\n",
    "        title_indices_set = set()\n",
    "        \n",
    "        for title in title_pattern.findall(self.text):\n",
    "            index = get_index_from_title(title)\n",
    "            if index is not None:\n",
    "                if index not in title_indices_set:\n",
    "                    title_indices_set.add(index)\n",
    "                    links.append(index)\n",
    "                    page_n_links[-1] += 1\n",
    "                    \n",
    "        if debugging:\n",
    "            if self.cnt >10000:\n",
    "                raise AbortException()\n",
    "                    \n",
    "    parse_wiki(on_page)\n",
    "        \n",
    "    links = np.array(links, dtype=np.int)\n",
    "    page_n_links = np.array(page_n_links, dtype=np.int)\n",
    "    np.save(\"pass2.links.npy\", links)\n",
    "    np.save(\"pass2.page_n_links.npy\", page_n_links)\n",
    "\n",
    "%timeit -r1 -n1 get_page_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6047510\n",
      "6047510\n",
      "170622100\n"
     ]
    }
   ],
   "source": [
    "links = np.load(\"pass2.links.npy\")\n",
    "page_n_links = np.load(\"pass2.page_n_links.npy\")\n",
    "\n",
    "assert(len(page_n_links) == len(page_title_indices))\n",
    "assert(page_n_links.sum() == len(links))\n",
    "\n",
    "print(len(page_n_links))\n",
    "print(len(page_titles))\n",
    "print(len(links))\n",
    "\n",
    "page_start = np.zeros(len(page_n_links), dtype=np.int)\n",
    "page_start[1:] = np.cumsum(page_n_links)[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## title模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_from_title(title):\n",
    "    index = get_index_from_title(title)\n",
    "    if index is not None:\n",
    "        s = page_start[index]\n",
    "        n = page_n_links[index]\n",
    "        return [get_title_from_index(i) for i in links[s:s+n] ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Logarithmic scale', 'Algorithm', 'Google Search', 'Web page', 'Web search engine', 'Larry Page', 'Google Patents', 'Network theory', 'Weighting', 'Hyperlink', 'Set (abstract data type)', 'World Wide Web', 'Link building', 'Webgraph', 'CNN', 'USA.gov', 'Recursion', 'Backlink', 'HITS algorithm', 'Jon Kleinberg', 'Teoma', 'Ask.com', 'CLEVER project', 'TrustRank', 'Google Hummingbird', 'Eigenvalues and eigenvectors', 'Scientometrics', 'Thomas L. Saaty', 'Analytic hierarchy process', 'Cognitive model', 'Baidu', 'Robin Li', 'The New York Times', 'Forbes', 'Sergey Brin', 'Stanford University', 'Rajeev Motwani', 'Terry Winograd', 'Google', 'Software patent', 'Citation analysis', 'Eugene Garfield', 'Hyper Search', 'Massimo Marchiori', 'University of Padua', 'Probability distribution', 'Matt Cutts', 'Markov chain', 'URL', 'Adjacency matrix', 'Stochastic matrix', 'Eigenvector centrality', 'Eigengap', 'Expected value', 'Wikipedia', 'Link farm', 'Trade secret', 'Power iteration', 'Steady state', 'Identity matrix', 'Perron–Frobenius theorem', 'Apache Spark', 'MATLAB', 'GNU Octave', 'Python (programming language)', 'Graph (abstract data type)', 'Bipartite graph', 'Random walk', 'Distributed algorithm', 'Google Toolbar', 'Danny Sullivan (technologist)', 'Search engine results page', 'Moz (marketing software)', 'Search engine optimization', 'Google Maps', 'Google Directory', 'HTTP 302', 'Meta element', 'Website spoofing', 'Nofollow', 'HTML attribute', 'Gaming the system', 'Citation', 'Sociometry', 'Attention economy', 'Institute for Scientific Information', 'Impact factor', 'Eigenfactor', 'SCImago Journal Rank', 'Neuroscience', 'Neuron', 'Twitter', 'Swiftype', 'Web crawler', 'Blogosphere', 'Scale-free network', 'Lexical semantics', 'Word-sense disambiguation', 'Semantic similarity', 'WordNet', 'Synonym ring', 'Link relation', 'Blog', 'Spamdexing', 'Spam in blogs', 'Search engine optimization metrics', 'Google Chrome', 'Domain Authority', 'EigenTrust', 'Google bombing', 'Google matrix', 'Google Panda', 'Google Penguin', 'VisualRank', 'Hilltop algorithm', 'Katz centrality', 'SimRank', 'CheiRank', 'Attention inequality']\n"
     ]
    }
   ],
   "source": [
    "print(get_links_from_title(\"PageRank\"))\n",
    "#print(get_links_from_title(\"Shanghai\"))\n",
    "#print(get_links_from_title(\"Beijing\"))\n",
    "#print(get_links_from_title(\"China\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迭代算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "\n",
    "@nb.njit\n",
    "def PR_update(links, page_n_links, PR, PR_out, d):\n",
    "    \n",
    "    n_pages = len(PR)\n",
    "    \n",
    "    for i in range(n_pages):\n",
    "        PR_out[i] = 0.0\n",
    "        \n",
    "    l = 0\n",
    "    for i in range(n_pages):\n",
    "        \n",
    "        n_links = page_n_links[i]\n",
    "        # this let the sum of the PR smaller than 1\n",
    "        if n_links > 0:\n",
    "            w =  PR[i] / n_links\n",
    "\n",
    "            for j in range(n_links):\n",
    "                PR_out[links[l]] += w\n",
    "                l += 1\n",
    "        else:\n",
    "#             PR_out[i] += PR[i]\n",
    "            pass\n",
    "            \n",
    "    # dump\n",
    "    dump = (1. - d)/n_pages\n",
    "    for i in range(n_pages):\n",
    "        PR_out[i] = PR_out[i]*d + dump\n",
    "        \n",
    "        \n",
    "def page_rank(links, page_n_links, n_iters, d=0.85, on_update=None):\n",
    "    \n",
    "    PR = np.ones(len(page_n_links))/len(page_n_links)\n",
    "    PR_out = np.empty(len(page_n_links))\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        PR_update(links, page_n_links, PR, PR_out, d)\n",
    "        \n",
    "        if on_update:\n",
    "            on_update(i, PR, PR_out)\n",
    "        \n",
    "        # swap\n",
    "        t = PR\n",
    "        PR = PR_out\n",
    "        PR_out = t\n",
    "        \n",
    "    return PR\n",
    "\n",
    "def on_update(i, PR, PR_new):\n",
    "    if i % 10 == 0:\n",
    "        norm = np.linalg.norm(PR - PR_new)\n",
    "        print(\"%3d: %f   \"%(i, norm), end=\"\\n\", flush=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: 0.009104   \n",
      " 10: 0.000013   \n",
      " 20: 0.000001   \n",
      " 30: 0.000000   \n"
     ]
    }
   ],
   "source": [
    "PR = page_rank(links, page_n_links, on_update=on_update, n_iters=40)\n",
    "PR = PR*len(PR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PR(title):\n",
    "    index = get_index_from_title(title)\n",
    "    if index is not None:\n",
    "        return PR[index]\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China                2922.7\n",
      "Beijing              726.1\n",
      "Hongkong             960.5\n",
      "Shanghai             405.5\n",
      "Taiwan               962.3\n",
      "Guangdong            221.3\n",
      "Hebei                127.7\n",
      "Taipei               226.6\n",
      "Shijiazhuang         28.7\n",
      "Shenzhen             106.0\n",
      "USA                  10132.7\n"
     ]
    }
   ],
   "source": [
    "regions = [\"China\", \"Beijing\", \"Hongkong\", \"Shanghai\", \"Taiwan\", \"Guangdong\", \"Hebei\", \"Taipei\", \"Shijiazhuang\", \"Shenzhen\", \"USA\"]\n",
    "for region in regions:\n",
    "    print(\"%-20s\"%region, \"%.1f\"%get_PR(region))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States                                                                                        10132.7\n",
      "World War II                                                                                         5060.9\n",
      "France                                                                                               4984.0\n",
      "Association football                                                                                 4720.8\n",
      "United Kingdom                                                                                       4572.3\n",
      "List of sovereign states                                                                             4529.6\n",
      "The New York Times                                                                                   4267.5\n",
      "Germany                                                                                              4254.5\n",
      "India                                                                                                4072.7\n",
      "New York City                                                                                        3432.1\n",
      "London                                                                                               3062.6\n",
      "Animal                                                                                               3055.0\n",
      "Australia                                                                                            3032.2\n",
      "Canada                                                                                               3008.1\n",
      "Italy                                                                                                2987.7\n",
      "China                                                                                                2922.7\n",
      "Catholic Church                                                                                      2898.4\n",
      "England                                                                                              2857.6\n",
      "Russia                                                                                               2842.6\n",
      "Iran                                                                                                 2840.5\n",
      "English language                                                                                     2751.0\n",
      "Japan                                                                                                2750.8\n",
      "World War I                                                                                          2732.6\n",
      "Arthropod                                                                                            2463.2\n",
      "Insect                                                                                               2449.5\n",
      "U.S. state                                                                                           2307.9\n",
      "Latin                                                                                                2269.7\n",
      "Spain                                                                                                2238.2\n",
      "Poland                                                                                               2107.4\n",
      "California                                                                                           2104.3\n",
      "National Register of Historic Places                                                                 2061.2\n",
      "Soviet Union                                                                                         2045.1\n",
      "The Guardian                                                                                         2033.2\n",
      "Paris                                                                                                2007.7\n",
      "Netherlands                                                                                          1984.5\n",
      "Brazil                                                                                               1910.5\n",
      "Washington, D.C.                                                                                     1878.7\n",
      "Europe                                                                                               1756.9\n",
      "Sweden                                                                                               1753.2\n",
      "Village                                                                                              1689.3\n",
      "Mexico                                                                                               1661.6\n",
      "Moth                                                                                                 1620.8\n",
      "Species                                                                                              1604.7\n",
      "Scotland                                                                                             1581.7\n",
      "Departments of France                                                                                1571.3\n",
      "Switzerland                                                                                          1567.1\n",
      "French language                                                                                      1552.0\n",
      "New Zealand                                                                                          1551.0\n",
      "New York (state)                                                                                     1547.0\n",
      "Genus                                                                                                1521.3\n",
      "BBC                                                                                                  1512.3\n",
      "Oxford University Press                                                                              1478.3\n",
      "Turkey                                                                                               1466.2\n",
      "Bakhsh                                                                                               1460.8\n",
      "Communes of France                                                                                   1455.5\n",
      "Los Angeles                                                                                          1441.8\n",
      "South Africa                                                                                         1426.4\n",
      "European Union                                                                                       1393.9\n",
      "Norway                                                                                               1391.6\n",
      "AllMusic                                                                                             1347.5\n",
      "United States Census Bureau                                                                          1309.2\n",
      "Chicago                                                                                              1304.5\n",
      "Democratic Party (United States)                                                                     1303.4\n",
      "Central European Time                                                                                1297.4\n",
      "Republican Party (United States)                                                                     1285.7\n",
      "Counties of Iran                                                                                     1285.7\n",
      "Provinces of Iran                                                                                    1283.6\n",
      "Belgium                                                                                              1262.7\n",
      "United Nations                                                                                       1244.4\n",
      "The Washington Post                                                                                  1238.3\n",
      "Philippines                                                                                          1213.1\n",
      "Romanization                                                                                         1203.4\n",
      "Lepidoptera                                                                                          1202.8\n",
      "Middle Ages                                                                                          1199.1\n",
      "Austria                                                                                              1188.8\n",
      "Romania                                                                                              1188.4\n",
      "German language                                                                                      1185.1\n",
      "Family (biology)                                                                                     1152.5\n",
      "Spanish language                                                                                     1152.0\n",
      "Central European Summer Time                                                                         1146.6\n",
      "Argentina                                                                                            1145.6\n",
      "Rome                                                                                                 1144.6\n",
      "Newspaper                                                                                            1139.7\n",
      "Dehestan (administrative division)                                                                   1138.4\n",
      "United States dollar                                                                                 1133.3\n",
      "Denmark                                                                                              1132.5\n",
      "Greece                                                                                               1125.9\n",
      "Geographic Names Information System                                                                  1122.4\n",
      "Ontario                                                                                              1111.2\n",
      "Christianity                                                                                         1109.3\n",
      "Indonesia                                                                                            1108.6\n",
      "Pakistan                                                                                             1108.4\n",
      "Cambridge University Press                                                                           1107.7\n",
      "Los Angeles Times                                                                                    1097.1\n",
      "Beetle                                                                                               1093.8\n",
      "American Civil War                                                                                   1092.5\n",
      "Israel                                                                                               1091.0\n",
      "Berlin                                                                                               1076.0\n",
      "Köppen climate classification                                                                        1070.4\n",
      "North America                                                                                        1064.8\n"
     ]
    }
   ],
   "source": [
    "def top100(PR):\n",
    "    n = 0\n",
    "    PR_I_SORT = np.argsort(PR)[::-1]\n",
    "    for j in PR_I_SORT:\n",
    "        title = page_titles[j]\n",
    "        print(\"%-100s\"%page_titles[j], \"%.1f\"%PR[j])\n",
    "        n += 1\n",
    "        if n == 100:\n",
    "            break\n",
    "    \n",
    "top100(PR)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": "24",
    "lenType": "16",
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
