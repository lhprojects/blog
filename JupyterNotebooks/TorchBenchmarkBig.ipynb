{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#模型\" data-toc-modified-id=\"模型-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>模型</a></span></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Results</a></span><ul class=\"toc-item\"><li><span><a href=\"#my-notebook-1\" data-toc-modified-id=\"my-notebook-1-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>my notebook 1</a></span><ul class=\"toc-item\"><li><span><a href=\"#External-HDD,-BUP-Slim-BK-+-USB3.0\" data-toc-modified-id=\"External-HDD,-BUP-Slim-BK-+-USB3.0-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>External HDD, BUP Slim BK + USB3.0</a></span></li></ul></li><li><span><a href=\"#my-notebook-2\" data-toc-modified-id=\"my-notebook-2-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>my notebook 2</a></span></li><li><span><a href=\"#my-desktop-2\" data-toc-modified-id=\"my-desktop-2-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>my desktop 2</a></span><ul class=\"toc-item\"><li><span><a href=\"#External-HDD,-BUP-Slim-BK-+-USB3.0\" data-toc-modified-id=\"External-HDD,-BUP-Slim-BK-+-USB3.0-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>External HDD, BUP Slim BK + USB3.0</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用CIFAR10数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % https://raw.githubusercontent.com/pytorch/examples/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 128, 3, 1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, 1, padding=1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(32*32//4*256, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc3(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "#     import torch.autograd.profiler as profiler\n",
    "#     with profiler.profile(record_shapes=True) as prof:\n",
    "#         with profiler.record_function(\"model_inference\"):\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                     epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                     100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "#     print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    \n",
    "    #print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def main(batch_size=32,test_batch_size=32):\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args = parser.parse_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': batch_size}\n",
    "    test_kwargs = {'batch_size': test_batch_size}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.CIFAR10 ('../data', train=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.CIFAR10 ('../data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    \n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3, 3, 3) torch.float32 0.0032958984375\n",
      "(32,) torch.float32 0.0001220703125\n",
      "(128, 32, 3, 3) torch.float32 0.140625\n",
      "(128,) torch.float32 0.00048828125\n",
      "(256, 128, 3, 3) torch.float32 1.125\n",
      "(256,) torch.float32 0.0009765625\n",
      "(512, 65536) torch.float32 128.0\n",
      "(512,) torch.float32 0.001953125\n",
      "(128, 512) torch.float32 0.25\n",
      "(128,) torch.float32 0.00048828125\n",
      "(10, 128) torch.float32 0.0048828125\n",
      "(10,) torch.float32 3.814697265625e-05\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import numpy as np\n",
    "import sys\n",
    "import psutil\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "def list_env():\n",
    "    print(platform.node())\n",
    "    print(platform.platform())\n",
    "    print(platform.machine())\n",
    "    try:\n",
    "        import cpuinfo\n",
    "        print(cpuinfo.get_cpu_info()[\"brand_raw\"])\n",
    "        print(\"cpu-count\", cpuinfo.get_cpu_info()[\"count\"])\n",
    "    except:\n",
    "        print(platform.processor())\n",
    "        print(\"cpu-count\", psutil.cpu_count())\n",
    "        \n",
    "    print(sys.version)\n",
    "    print(torch.__version__)\n",
    "    print(torch.__config__.show())\n",
    "    print(torch.__config__.parallel_info())\n",
    "\n",
    "    \n",
    "    print(\"gpu available:\", torch.cuda.is_available())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(torch.cuda.get_device_name(i))\n",
    "        \n",
    "        \n",
    "def model_summary():            \n",
    "    model = Net()\n",
    "    for param in model.parameters():\n",
    "        print(tuple(param.size()), param.dtype,\n",
    "              np.array(param.size()).prod()*4/(1024*1024))\n",
    "\n",
    "def download_data():\n",
    "    datasets.CIFAR10('../data', download=True)\n",
    "    \n",
    "def benchmark(try_gpu=True, try_cpu=False):\n",
    "\n",
    "    if try_gpu and torch.cuda.is_available():        \n",
    "        print(\"device=gpu\")\n",
    "        psutil.cpu_percent(percpu=True)\n",
    "        argv = sys.argv\n",
    "        sys.argv = [\"\", \"--epochs\", \"1\"]\n",
    "        %timeit -r1 -n1 main()\n",
    "        sys.argv = argv\n",
    "        print(\"cpu-percent:\", psutil.cpu_percent(percpu=True))\n",
    "        print()\n",
    "        \n",
    "    if try_cpu:\n",
    "        print(\"device=cpu\")\n",
    "        psutil.cpu_percent(percpu=True)\n",
    "        argv = sys.argv\n",
    "        sys.argv = [\"\", \"--epochs\",\"1\", \"--no-cuda\"]\n",
    "        %timeit -r1 -n1 main()\n",
    "        sys.argv = argv\n",
    "        print(\"cpu-percent:\", psutil.cpu_percent(percpu=True))\n",
    "        print()\n",
    "    \n",
    "model_summary()\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## my notebook 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESKTOP-U61NOQ2\n",
      "Windows-10-10.0.18362-SP0\n",
      "AMD64\n",
      "Intel64 Family 6 Model 142 Stepping 10, GenuineIntel\n",
      "cpu-count 8\n",
      "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\n",
      "1.7.1\n",
      "PyTorch built with:\n",
      "  - C++ Version: 199711\n",
      "  - MSVC 192729112\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191125 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)\n",
      "  - OpenMP 2019\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 10.2\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37\n",
      "  - CuDNN 7.6.5\n",
      "  - Magma 2.5.4\n",
      "  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj -openmp:experimental -DNDEBUG -DUSE_FBGEMM -DUSE_VULKAN_WRAPPER, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, \n",
      "\n",
      "ATen/Parallel:\n",
      "\tat::get_num_threads() : 4\n",
      "\tat::get_num_interop_threads() : 4\n",
      "OpenMP 2019\n",
      "\tomp_get_max_threads() : 4\n",
      "Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191125 for Intel(R) 64 architecture applications\n",
      "\tmkl_get_max_threads() : 4\n",
      "Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)\n",
      "std::thread::hardware_concurrency() : 8\n",
      "Environment variables:\n",
      "\tOMP_NUM_THREADS : [not set]\n",
      "\tMKL_NUM_THREADS : [not set]\n",
      "ATen parallel backend: OpenMP\n",
      "\n",
      "gpu available: True\n",
      "GeForce MX250\n"
     ]
    }
   ],
   "source": [
    "list_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External HDD, BUP Slim BK + USB3.0\n",
    "\n",
    "cuda is running 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=gpu\n",
      "Test set: Average loss: 1.1687, Accuracy: 5827/10000 (58%)\n",
      "4min 36s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "cpu-percent: [19.9, 3.2, 11.9, 4.4, 8.6, 5.6, 5.9, 71.6]\n",
      "\n",
      "device=gpu\n",
      "Test set: Average loss: 1.1561, Accuracy: 6020/10000 (60%)\n",
      "4min 33s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "cpu-percent: [12.5, 2.3, 7.7, 4.8, 3.5, 2.1, 3.3, 80.6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark()\n",
    "benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## my notebook 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## my desktop 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESKTOP-0J5HGI3\n",
      "Windows-10-10.0.19041-SP0\n",
      "AMD64\n",
      "AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD\n",
      "cpu-count 16\n",
      "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\n",
      "1.7.1\n",
      "PyTorch built with:\n",
      "  - C++ Version: 199711\n",
      "  - MSVC 192729112\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191125 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)\n",
      "  - OpenMP 2019\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.0\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37\n",
      "  - CuDNN 8.0.4\n",
      "  - Magma 2.5.4\n",
      "  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj -openmp:experimental -DNDEBUG -DUSE_FBGEMM -DUSE_VULKAN_WRAPPER, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, \n",
      "\n",
      "ATen/Parallel:\n",
      "\tat::get_num_threads() : 8\n",
      "\tat::get_num_interop_threads() : 8\n",
      "OpenMP 2019\n",
      "\tomp_get_max_threads() : 8\n",
      "Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191125 for Intel(R) 64 architecture applications\n",
      "\tmkl_get_max_threads() : 8\n",
      "Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)\n",
      "std::thread::hardware_concurrency() : 16\n",
      "Environment variables:\n",
      "\tOMP_NUM_THREADS : [not set]\n",
      "\tMKL_NUM_THREADS : [not set]\n",
      "ATen parallel backend: OpenMP\n",
      "\n",
      "gpu available: True\n",
      "GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "list_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External HDD, BUP Slim BK + USB3.0\n",
    "cuda is runnig 98%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=gpu\n",
      "Test set: Average loss: 1.1820, Accuracy: 5897/10000 (59%)\n",
      "39.7 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "cpu-percent: [8.3, 1.2, 3.8, 6.6, 2.2, 10.7, 6.5, 9.7, 4.4, 2.2, 4.0, 1.2, 2.3, 5.6, 2.2, 80.2]\n",
      "\n",
      "device=gpu\n",
      "Test set: Average loss: 1.1820, Accuracy: 5897/10000 (59%)\n",
      "37.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "cpu-percent: [7.6, 1.7, 3.0, 10.9, 6.8, 3.1, 3.7, 5.9, 1.9, 2.4, 6.5, 3.1, 3.6, 6.3, 2.2, 87.5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark()\n",
    "benchmark()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "789px",
    "left": "101px",
    "top": "220px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": "24",
    "lenType": "16",
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
