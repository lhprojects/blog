{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#模型\" data-toc-modified-id=\"模型-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>模型</a></span></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Results</a></span><ul class=\"toc-item\"><li><span><a href=\"#my-notebook-1\" data-toc-modified-id=\"my-notebook-1-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>my notebook 1</a></span><ul class=\"toc-item\"><li><span><a href=\"#External-HDD,-BUP-Slim-BK-+-USB3.0\" data-toc-modified-id=\"External-HDD,-BUP-Slim-BK-+-USB3.0-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>External HDD, BUP Slim BK + USB3.0</a></span></li><li><span><a href=\"#Internal-SSD\" data-toc-modified-id=\"Internal-SSD-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Internal SSD</a></span></li></ul></li><li><span><a href=\"#my-notebook-2\" data-toc-modified-id=\"my-notebook-2-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>my notebook 2</a></span></li><li><span><a href=\"#my-desktop-2\" data-toc-modified-id=\"my-desktop-2-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>my desktop 2</a></span><ul class=\"toc-item\"><li><span><a href=\"#External-HDD,-BUP-Slim-BK-+-USB3.0\" data-toc-modified-id=\"External-HDD,-BUP-Slim-BK-+-USB3.0-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>External HDD, BUP Slim BK + USB3.0</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用torch官方的benchmark例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % https://raw.githubusercontent.com/pytorch/examples/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 128, 3, 1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 512, 3, 1, padding=1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(32*32//4*512, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc3(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "#     import torch.autograd.profiler as profiler\n",
    "#     with profiler.profile(record_shapes=True) as prof:\n",
    "#         with profiler.record_function(\"model_inference\"):\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                     epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                     100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "#     print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    \n",
    "    #print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args = parser.parse_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.CIFAR10 ('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.CIFAR10 ('../data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    \n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None True torch.Size([32, 3, 3, 3])\n",
      "None True torch.Size([32])\n",
      "None True torch.Size([128, 32, 3, 3])\n",
      "None True torch.Size([128])\n",
      "None True torch.Size([512, 128, 3, 3])\n",
      "None True torch.Size([512])\n",
      "None True torch.Size([1024, 131072])\n",
      "None True torch.Size([1024])\n",
      "None True torch.Size([128, 1024])\n",
      "None True torch.Size([128])\n",
      "None True torch.Size([10, 128])\n",
      "None True torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import sys\n",
    "import psutil\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "def list_env():\n",
    "    print(platform.node())\n",
    "    print(platform.platform())\n",
    "    print(platform.machine())\n",
    "    try:\n",
    "        import cpuinfo\n",
    "        print(cpuinfo.get_cpu_info()[\"brand_raw\"])\n",
    "        print(\"cpu-count\", cpuinfo.get_cpu_info()[\"count\"])\n",
    "    except:\n",
    "        print(platform.processor())\n",
    "        print(\"cpu-count\", psutil.cpu_count())\n",
    "        \n",
    "    print(sys.version)\n",
    "    print(torch.__version__)\n",
    "    print(torch.__config__.show())\n",
    "    print(torch.__config__.parallel_info())\n",
    "\n",
    "    \n",
    "    print(\"gpu available:\", torch.cuda.is_available())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(torch.cuda.get_device_name(i))\n",
    "        \n",
    "        \n",
    "def model_summary():            \n",
    "    model = Net()\n",
    "    for param in model.parameters():\n",
    "        print(param.name, param.requires_grad,  param.size())\n",
    "\n",
    "def download_data():\n",
    "    dataset1 = datasets.MNIST('../data', train= True, download=True)\n",
    "    dataset2 = datasets.MNIST('../data', train=False, download=True)\n",
    "    \n",
    "def benchmark(try_gpu=True, try_cpu=False):\n",
    "\n",
    "    if try_gpu and torch.cuda.is_available():        \n",
    "        print(\"device=gpu\")\n",
    "        psutil.cpu_percent(percpu=True)\n",
    "        argv = sys.argv\n",
    "        sys.argv = [\"\", \"--epochs\", \"1\", \"--test-batch-size\", \"64\"]\n",
    "        %timeit -r1 -n1 main()\n",
    "        sys.argv = argv\n",
    "        print(\"cpu-percent:\", psutil.cpu_percent(percpu=True))\n",
    "        print()\n",
    "        \n",
    "    if try_cpu:\n",
    "        print(\"device=cpu\")\n",
    "        psutil.cpu_percent(percpu=True)\n",
    "        argv = sys.argv\n",
    "        sys.argv = [\"\", \"--epochs\",\"1\", \"--no-cuda\", \"--test-batch-size\", \"64\"]\n",
    "        %timeit -r1 -n1 main()\n",
    "        sys.argv = argv\n",
    "        print(\"cpu-percent:\", psutil.cpu_percent(percpu=True))\n",
    "        print()\n",
    "    \n",
    "model_summary()\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## my notebook 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESKTOP-U61NOQ2\n",
      "Windows-10-10.0.18362-SP0\n",
      "AMD64\n",
      "Intel64 Family 6 Model 142 Stepping 10, GenuineIntel\n",
      "cpu-count 8\n",
      "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\n",
      "1.7.1\n",
      "PyTorch built with:\n",
      "  - C++ Version: 199711\n",
      "  - MSVC 192729112\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191125 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)\n",
      "  - OpenMP 2019\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 10.2\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37\n",
      "  - CuDNN 7.6.5\n",
      "  - Magma 2.5.4\n",
      "  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj -openmp:experimental -DNDEBUG -DUSE_FBGEMM -DUSE_VULKAN_WRAPPER, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, \n",
      "\n",
      "ATen/Parallel:\n",
      "\tat::get_num_threads() : 4\n",
      "\tat::get_num_interop_threads() : 4\n",
      "OpenMP 2019\n",
      "\tomp_get_max_threads() : 4\n",
      "Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191125 for Intel(R) 64 architecture applications\n",
      "\tmkl_get_max_threads() : 4\n",
      "Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)\n",
      "std::thread::hardware_concurrency() : 8\n",
      "Environment variables:\n",
      "\tOMP_NUM_THREADS : [not set]\n",
      "\tMKL_NUM_THREADS : [not set]\n",
      "ATen parallel backend: OpenMP\n",
      "\n",
      "gpu available: True\n",
      "GeForce MX250\n"
     ]
    }
   ],
   "source": [
    "list_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External HDD, BUP Slim BK + USB3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=gpu\n",
      "Test set: Average loss: 0.0438, Accuracy: 9856/10000 (99%)\n",
      "22.7 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "cpu-percent: [34.9, 10.7, 40.5, 15.2, 32.1, 10.9, 18.7, 74.4]\n",
      "\n",
      "device=cpu\n",
      "Test set: Average loss: 0.0460, Accuracy: 9853/10000 (99%)\n",
      "2min 12s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "cpu-percent: [46.0, 57.1, 60.2, 65.3, 46.0, 52.5, 60.1, 72.1]\n",
      "\n",
      "device=gpu\n",
      "Test set: Average loss: 0.0413, Accuracy: 9869/10000 (99%)\n",
      "20.1 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "cpu-percent: [31.2, 9.9, 35.1, 10.6, 31.6, 8.9, 17.2, 77.7]\n",
      "\n",
      "device=cpu\n",
      "Test set: Average loss: 0.0460, Accuracy: 9853/10000 (99%)\n",
      "2min 15s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "cpu-percent: [50.2, 60.3, 62.5, 69.5, 50.1, 52.9, 61.1, 74.2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark()\n",
    "benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=gpu\n",
      "Test set: Average loss: 0.0440, Accuracy: 9858/10000 (99%)\n",
      "22.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "cpu-percent: [22.4, 4.5, 46.0, 5.9, 19.5, 3.6, 8.6, 82.5]\n",
      "\n",
      "device=cpu\n",
      "Test set: Average loss: 0.0460, Accuracy: 9853/10000 (99%)\n",
      "2min 16s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "cpu-percent: [53.1, 61.2, 66.5, 71.0, 48.1, 51.9, 59.5, 72.9]\n",
      "\n",
      "device=gpu\n",
      "Test set: Average loss: 0.0450, Accuracy: 9853/10000 (99%)\n",
      "20.2 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "cpu-percent: [32.6, 10.4, 39.7, 13.9, 31.1, 9.5, 17.1, 78.5]\n",
      "\n",
      "device=cpu\n",
      "Test set: Average loss: 0.0460, Accuracy: 9853/10000 (99%)\n",
      "2min 15s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "cpu-percent: [48.2, 59.2, 61.1, 73.1, 50.4, 52.0, 60.6, 73.6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark()\n",
    "benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## my notebook 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESKTOP-JPH34GE\n",
      "Windows-10-10.0.17134-SP0\n",
      "Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\n",
      "cpu-count 8\n",
      "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\n",
      "1.7.1\n",
      "gpu available: True\n",
      "GeForce GTX 850M\n",
      "\n",
      "device=gpu\n",
      "Test set: Average loss: 0.0441, Accuracy: 9860/10000 (99%)\n",
      "31.8 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "cpu-percent: [33.8, 14.1, 23.1, 8.2, 12.1, 9.7, 5.4, 61.7]\n",
      "\n",
      "device=cpu\n",
      "Test set: Average loss: 0.0460, Accuracy: 9853/10000 (99%)\n",
      "2min 44s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "cpu-percent: [65.4, 45.8, 61.2, 59.0, 53.5, 59.0, 55.6, 69.9]\n"
     ]
    }
   ],
   "source": [
    "benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## my desktop 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESKTOP-0J5HGI3\n",
      "Windows-10-10.0.19041-SP0\n",
      "AMD64\n",
      "AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD\n",
      "cpu-count 16\n",
      "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\n",
      "1.7.1\n",
      "PyTorch built with:\n",
      "  - C++ Version: 199711\n",
      "  - MSVC 192729112\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191125 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)\n",
      "  - OpenMP 2019\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.0\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37\n",
      "  - CuDNN 8.0.4\n",
      "  - Magma 2.5.4\n",
      "  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj -openmp:experimental -DNDEBUG -DUSE_FBGEMM -DUSE_VULKAN_WRAPPER, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, \n",
      "\n",
      "ATen/Parallel:\n",
      "\tat::get_num_threads() : 8\n",
      "\tat::get_num_interop_threads() : 8\n",
      "OpenMP 2019\n",
      "\tomp_get_max_threads() : 8\n",
      "Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191125 for Intel(R) 64 architecture applications\n",
      "\tmkl_get_max_threads() : 8\n",
      "Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)\n",
      "std::thread::hardware_concurrency() : 16\n",
      "Environment variables:\n",
      "\tOMP_NUM_THREADS : [not set]\n",
      "\tMKL_NUM_THREADS : [not set]\n",
      "ATen parallel backend: OpenMP\n",
      "\n",
      "gpu available: True\n",
      "GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "list_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External HDD, BUP Slim BK + USB3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=gpu\n",
      "Files already downloaded and verified\n",
      "Test set: Average loss: 1.2363, Accuracy: 5662/10000 (57%)\n",
      "1min 5s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "cpu-percent: [6.1, 2.5, 7.0, 4.1, 3.0, 8.5, 6.9, 9.5, 3.7, 2.7, 4.5, 2.5, 3.5, 3.9, 3.6, 89.4]\n",
      "\n",
      "device=gpu\n",
      "Files already downloaded and verified\n",
      "Test set: Average loss: 1.2316, Accuracy: 5727/10000 (57%)\n",
      "1min 2s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "cpu-percent: [2.9, 0.7, 1.8, 6.5, 3.1, 1.7, 2.8, 3.2, 0.8, 0.8, 2.4, 1.0, 0.5, 1.9, 0.7, 95.7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark()\n",
    "benchmark()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "789px",
    "left": "101px",
    "top": "220px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": "24",
    "lenType": "16",
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
